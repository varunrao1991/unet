{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1606203804009,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "PLNZ8zakUs6W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1439,
     "status": "ok",
     "timestamp": 1606203670303,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "yGFDYtaqByBF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb2bgr(input_img):\n",
    "    bgr_image = np.zeros_like(input_img)\n",
    "    bgr_image[:,:,0] = input_img[:,:,2] # R channel to B channel\n",
    "    bgr_image[:,:,1] = input_img[:,:,1] # G channel remains the same\n",
    "    bgr_image[:,:,2] = input_img[:,:,0] # B channel to R channel\n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4190,
     "status": "ok",
     "timestamp": 1606203813346,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "SnRR09qRUv9S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "import random \n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 320,240\n",
    "base_data_dir = \"A:/data/hackathon_2023_1\"\n",
    "model_dir = \"A:/models/tool_segmentation\"\n",
    "sample_test_dir = \"A:/python_ai_projects/tool_segmentation/sample_data\"\n",
    "image_directory_name = \"foreground\"\n",
    "mask_directory_name = \"mask2\"\n",
    "background_dir_name = \"background\"\n",
    "model_name = \"mobilenet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiODc8j9cPj6"
   },
   "source": [
    "# **Directory reference, data augmentor, data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2261,
     "status": "ok",
     "timestamp": 1606203813355,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "xZthNfffVUY6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_bg_change_list_file = os.path.join(base_data_dir, 'no_background_change.txt')\n",
    "file_names_to_exclude_from_bg_change = []\n",
    "if os.path.exists(no_bg_change_list_file):\n",
    "    with open(no_bg_change_list_file, 'r') as f:\n",
    "        # Read the contents of the file into a list of strings\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Strip the newline character from each line\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "\n",
    "    file_names_to_exclude_from_bg_change = lines\n",
    "print(file_names_to_exclude_from_bg_change)\n",
    "\n",
    "def revert_preprocess(in_im):\n",
    "    input_im = in_im.copy()\n",
    "    input_im += 1\n",
    "    input_im *= 127.5\n",
    "    return input_im.astype(np.uint8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "def activator_labels(images, augmenter, parents, default):\n",
    "    if augmenter.name in [\"AddToHueAndSaturation\", \"GaussianBlur\"]: # \"noaugment\"\n",
    "        return False\n",
    "    else:\n",
    "        return default\n",
    "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "            [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.GaussianBlur(sigma=(0,0.05), name= \"GaussianBlur\"),\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "            # crop images by -5% to 10% of their height/width\n",
    "            sometimes(iaa.CropAndPad(\n",
    "                percent=(-0.1, 0.1),\n",
    "                pad_mode=ia.ALL,\n",
    "                pad_cval=0\n",
    "            )),\n",
    "            sometimes(iaa.Affine(\n",
    "                scale=(0.8, 1.2), # scale images to 90-110% of their size, individually per axis\n",
    "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n",
    "\n",
    "                rotate=(-5, 5), # rotate by -5 to +5 degrees\n",
    "                #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "                #mode=ia.ALL, # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "                shear=(-1, 1) # shear by -16 to +16 degrees\n",
    "            )),\n",
    "            iaa.Resize({\"height\": h, \"width\": w}),\n",
    "            iaa.AddToHueAndSaturation((-5, 5), name= \"AddToHueAndSaturation\"), # change the hue and saturation by up to 20\n",
    "            iaa.Multiply((1, 1.5))\n",
    "            ],\n",
    "            random_order=False\n",
    "        )\n",
    "hooks_labels = ia.HooksImages(activator=activator_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2552,
     "status": "ok",
     "timestamp": 1606203817958,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "My7rnnciXkJH",
    "outputId": "dec61c8d-e53c-431e-8681-e3c65d3e0047"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "imag_paths = sorted(glob.glob(os.path.join(base_data_dir, image_directory_name + \"/*.jpg\"), recursive=True) + glob.glob(os.path.join(base_data_dir, image_directory_name + \"/*.bmp\"), recursive=True))\n",
    "mask_paths = sorted(glob.glob(os.path.join(base_data_dir, mask_directory_name + \"/*.png\"), recursive=True))\n",
    "bkgd_paths = sorted(glob.glob(os.path.join(base_data_dir, background_dir_name + \"/*.jpg\"), recursive=True) + glob.glob(os.path.join(base_data_dir, background_dir_name + \"/*.bmp\"), recursive=True))\n",
    "\n",
    "print(f'Total Train Images : {len(imag_paths)}')\n",
    "print(f'Total Mask Image : {len(mask_paths)}')\n",
    "print(f'Total background images : {len(bkgd_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1606204000629,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "c8HsqwowWnFz",
    "outputId": "cbb35091-5b2c-48b9-8576-32072a75b237"
   },
   "outputs": [],
   "source": [
    "train_imag_paths, test_imag_paths, train_mask_paths, test_mask_paths = train_test_split(imag_paths, mask_paths, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6dd4aa10b57e46ac9cb98bb92100c728",
      "480ceb01acd74b91b57950f80b557e1e",
      "f922376e800e4683b455f45b2eb9d9c8",
      "07ca0455627c47a78b5a99c64c285abd",
      "951347637fa544d790335e75bd622fac",
      "19bd521b63ea4fbaaccb116fcd021a70",
      "c5c70e40ef4e4006b90c03d85f04ca97",
      "73d95f2e29c448bda8505e09581e1c0e"
     ]
    },
    "executionInfo": {
     "elapsed": 176823,
     "status": "ok",
     "timestamp": 1606203994349,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "uHmA_NWTWV0S",
    "outputId": "dda2b3c6-3bb3-423b-9d63-b3090a42c0a5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 3\n",
    "class ImageSource:\n",
    "    def __init__(self, imag_paths, mask_paths, bkgd_paths, background_aug = False, backgroundchangeProbability = 0.25):\n",
    "        self.imges = np.zeros((len(imag_paths), h, w, 3), dtype=np.float32)\n",
    "        self.masks = np.zeros((len(mask_paths), h, w, num_classes), dtype=np.float32)\n",
    "        self.backgroundChangeProbability = backgroundchangeProbability\n",
    "\n",
    "        self.imag_paths = imag_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.bkgd_paths = bkgd_paths\n",
    "        \n",
    "        self.total_images = len(imag_paths)\n",
    "\n",
    "        self.max_blur_size = 10\n",
    "        self.min_blur_size = 1\n",
    "        self.max_circle_size = 0.11\n",
    "        \n",
    "        self.background_aug = background_aug\n",
    "        self.find_class_weights()\n",
    "        self.generate_augmented_images()\n",
    "\n",
    "    def preprocess_input(self, input_im):\n",
    "        return preprocess_input(input_im)\n",
    "        \n",
    "    def revert_preprocess(self, in_im):\n",
    "        return revert_preprocess(in_im)\n",
    "\n",
    "    def generate_augmented_images(self):\n",
    "        class_weights = [0 for i in range(num_classes)]\n",
    "        for n, (image_path, mask_path) in tqdm(enumerate(zip(self.imag_paths, self.mask_paths))):\n",
    "            #print(os.path.split(image_path)[1], os.path.split(mask_path)[1])\n",
    "            random_background_image_index = random.randint(0, len(bkgd_paths) - 1)\n",
    "\n",
    "            imag_data = img_to_array(load_img(image_path)).astype(np.uint8)\n",
    "            mask_data = img_to_array(load_img(mask_path, color_mode = \"grayscale\")).astype(np.uint8).squeeze()\n",
    "\n",
    "            seq_deterministic = seq.to_deterministic() \n",
    "            imag_data_augmented = seq_deterministic.augment_image(imag_data)\n",
    "            mask_data_augmented = seq_deterministic.augment_image(mask_data, hooks=hooks_labels)\n",
    "\n",
    "            image_filename = os.path.split(image_path)[1]\n",
    "            exclude_file = image_filename in file_names_to_exclude_from_bg_change\n",
    "            has_mask = mask_data.max() > 0\n",
    "            changeBackground = self.backgroundChangeProbability > random.random()\n",
    "            if self.background_aug and has_mask and not exclude_file and changeBackground:\n",
    "                bkgd_path = self.bkgd_paths[random_background_image_index]\n",
    "                bkgd_data = img_to_array(load_img(bkgd_path)).astype(np.uint8)\n",
    "                bkgd_data_augmented = seq_deterministic.augment_image(bkgd_data)\n",
    "                expanded_mask = np.expand_dims(mask_data_augmented == 0, -1)\n",
    "                mask = np.repeat(expanded_mask, 3, axis=-1)\n",
    "                imag_data_augmented[mask] = bkgd_data_augmented[mask]\n",
    "            image_blended_float = imag_data_augmented.astype(np.float32)\n",
    "            \n",
    "            self.imges[n] = self.preprocess_input(image_blended_float)\n",
    "            mask_3d = to_categorical(mask_data_augmented, num_classes)\n",
    "            self.masks[n] = mask_3d.astype(np.float32)\n",
    "\n",
    "    def find_class_weights(self):\n",
    "        class_weights = [0 for i in range(num_classes)]\n",
    "        for n, mask_path in tqdm(enumerate(self.mask_paths)):\n",
    "            mask_data = img_to_array(load_img(mask_path, color_mode = \"grayscale\")).astype(np.uint8).squeeze()\n",
    "            total_pixels = mask_data.shape[0] * mask_data.shape[1]\n",
    "            for i in range(num_classes):\n",
    "                class_weight = np.count_nonzero(mask_data == i) / total_pixels\n",
    "                class_weights[i] += class_weight\n",
    "                \n",
    "        self.class_weights = [1 - (class_weights[i] / self.total_images) for i in range(num_classes)]\n",
    "        print(self.class_weights)\n",
    "\n",
    "    def display_images(self, display_all = False):       \n",
    "        indices = []\n",
    "        if display_all:\n",
    "            indices = range(0,len(self.imges)-1)\n",
    "        else:\n",
    "            random_index = random.randint(0, len(self.imges)-1)\n",
    "            indices = range(random_index,random_index+1)\n",
    "            \n",
    "        for indice in indices:\n",
    "\n",
    "            selected_image = self.imges[indice]\n",
    "            selected_mask = np.argmax(self.masks[indice], axis=2)\n",
    "            has_mask = selected_image.max() > 0\n",
    "\n",
    "            fig, (input_figure, output_figure1, output_figure2, output_figure3) = plt.subplots(1, 4, figsize = (20, 15))\n",
    "            reverted_preprocessed_image = self.revert_preprocess(selected_image)\n",
    "\n",
    "            input_figure.imshow(reverted_preprocessed_image)\n",
    "            if has_mask: # if salt\n",
    "                # draw a boundary(contour) in the original image separating salt and non-salt areas\n",
    "                input_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "\n",
    "            input_figure.set_title('Image')\n",
    "            input_figure.set_axis_off()\n",
    "            output_figure1.imshow(selected_mask.squeeze() > 0, cmap = 'gray')\n",
    "            output_figure1.set_title('Mask Image1')\n",
    "            output_figure1.set_axis_off()   \n",
    "            output_figure2.imshow(selected_mask.squeeze() == 1, cmap = 'gray')\n",
    "            output_figure2.set_title('Mask Image2')\n",
    "            output_figure2.set_axis_off()   \n",
    "            output_figure3.imshow(selected_mask.squeeze() == 2, cmap = 'gray')\n",
    "            output_figure3.set_title('Mask Image3')\n",
    "            output_figure3.set_axis_off()       \n",
    "        \n",
    "\n",
    "    def display_images_pred(self, predict_func):\n",
    "        random_index = random.randint(0, len(self.imges)-1)\n",
    "            \n",
    "        selected_image = self.imges[random_index]\n",
    "        selected_mask = np.argmax(self.masks[random_index], axis=-1)\n",
    "        has_mask = selected_image.max() > 0\n",
    "\n",
    "        fig, (input_figure, output_figure, pred_figure) = plt.subplots(1, 3, figsize = (20, 15))\n",
    "        reverted_preprocessed_image = self.revert_preprocess(selected_image)\n",
    "\n",
    "        input_figure.imshow(reverted_preprocessed_image)\n",
    "        if has_mask:\n",
    "            input_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "\n",
    "        input_figure.set_title('Image')\n",
    "        input_figure.set_axis_off()\n",
    "\n",
    "        output_figure.imshow(selected_mask.squeeze(), cmap = 'gray')\n",
    "        if has_mask:\n",
    "            output_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "        output_figure.set_title('Mask Image')\n",
    "        output_figure.set_axis_off()   \n",
    "\n",
    "        pred = np.argmax(predict_func(selected_image), axis=-1)\n",
    "        pred_figure.imshow(pred.squeeze(), cmap = 'gray')\n",
    "        pred_figure.set_title('Predicted Image')\n",
    "        pred_figure.set_axis_off()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcYO1QSucdMy"
   },
   "source": [
    "# **Instantiate validation and training sources; visualize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "executionInfo": {
     "elapsed": 3742,
     "status": "ok",
     "timestamp": 1606204006196,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "9zTCOdJvcF1C",
    "outputId": "d204f2af-990a-4868-922a-59e965177ca5"
   },
   "outputs": [],
   "source": [
    "train_source = ImageSource(train_imag_paths, train_mask_paths, bkgd_paths, background_aug = True)\n",
    "test_source = ImageSource(test_imag_paths, test_mask_paths, bkgd_paths, background_aug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)\n",
    "train_source.display_images(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_source.display_images(False)\n",
    "test_source.display_images(False)\n",
    "test_source.display_images(False)\n",
    "test_source.display_images(False)\n",
    "test_source.display_images(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create upsample layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "import tensorflow.keras as keras\n",
    "#-------------------------------------------------------------------------------\n",
    "def DepthwisePointWise(inputs, filters, name, kernel_size=(3, 3), strides=(1, 1)):\n",
    "    depthwise = DepthwiseConv2D(kernel_size=kernel_size, strides=strides, padding='same', depth_multiplier=1, name=f'{name}_depth')(inputs)\n",
    "    pointwise = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), padding='same', name=f'{name}_point')(depthwise)\n",
    "    return pointwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNET model function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def UnetMobilenetModel(image_width, image_height, num_classes = 2):\n",
    "    inputs = Input((image_height, image_width, 3), name=\"mobilenet\")\n",
    "    \n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n",
    "    \n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    skip_connection_names = [\"mobilenet\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = Conv2DTranspose(filters=f[-i], kernel_size=(2,2), strides=(2,2), padding='same', use_bias = False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = DepthwisePointWise(x, f[-i], name=f'depth1_{i}')\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = DepthwisePointWise(x, f[-i], name=f'depth2_{i}')\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = DepthwisePointWise(x, num_classes, name='final')\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetMobilenetModel(w, h, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss function and training parameters; Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1606204194502,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "v6vsC6vVjwe3",
    "outputId": "b4b0fe8a-a6a3-4ba7-c9f2-1e65e22e94ea"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHES = 100000\n",
    "RELOAD_ON_EVERY = 10000\n",
    "EPOCH_PATIENCE = 1000\n",
    "LEARNING_RATE_PATIENCE = 100\n",
    "LERNING_RATE = 0.000001\n",
    "\n",
    "K.clear_session()\n",
    "class_weights = train_source.class_weights\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    # convert class weights dictionary to tensor\n",
    "    class_weights_tensor = K.constant(class_weights)\n",
    "    \n",
    "    # calculate the cross-entropy loss for each sample\n",
    "    per_sample_loss = categorical_crossentropy(y_true * class_weights_tensor, y_pred * class_weights_tensor)\n",
    "    # calculate the weighted loss for each sample\n",
    "    weighted_loss = K.sum(per_sample_loss)\n",
    "    \n",
    "    return weighted_loss\n",
    "\n",
    "class_weights = train_source.class_weights\n",
    "\n",
    "metrics = [\"accuracy\", tf.keras.metrics.AUC()]\n",
    "model.compile(optimizer=Adam(learning_rate=LERNING_RATE), loss=weighted_categorical_crossentropy, metrics=metrics)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "               \n",
    "class UpdateDataSample(Callback):\n",
    "    def __init__(self, train_data_source, batch_size = 4, update_frequency=10):\n",
    "        super(UpdateDataSample, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data_source = train_data_source\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch + 1) % self.update_frequency == 0:\n",
    "            self.train_data_source.generate_augmented_images()\n",
    "            num_batches = self.train_data_source.total_images // self.batch_size\n",
    "            for i in range(num_batches):\n",
    "                batch_x = self.train_data_source.imges[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                batch_y = self.train_data_source.masks[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                self.model.train_on_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=EPOCH_PATIENCE, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LEARNING_RATE_PATIENCE, min_lr=0.0001, verbose=1),\n",
    "    ModelCheckpoint(f'{model_name}.h5', \n",
    "                    verbose=1, \n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True),\n",
    "    CSVLogger(f\"{model_name}.csv\"),\n",
    "    TensorBoard(log_dir=f'./{model_name}_logs'),\n",
    "    UpdateDataSample(train_source, BATCH_SIZE, RELOAD_ON_EVERY)\n",
    "]\n",
    "\n",
    "if os.path.exists(f'{model_name}.h5'):\n",
    "    print(\"Existing model loaded\", model_name)\n",
    "    model.load_weights(f'{model_name}.h5')\n",
    "\n",
    "results = model.fit(train_source.imges, train_source.masks, batch_size = BATCH_SIZE, epochs=EPOCHES, callbacks=callbacks, validation_data=(test_source.imges, test_source.masks), use_multiprocessing=True)\n",
    "\n",
    "df_result = pd.DataFrame(results.history)\n",
    "df_result.sort_values('val_loss', ascending=True, inplace = True)\n",
    "df_result\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.plot(results.history[\"val_accuracy\"], label=\"val_Accuracy\")\n",
    "plt.plot(np.argmax(results.history[\"val_accuracy\"]), np.max(results.history[\"val_accuracy\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoTtCHM-qEBs"
   },
   "source": [
    "# **Inference; Dump model to use in c++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1606204258492,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "w3T22zhYpuaW"
   },
   "outputs": [],
   "source": [
    "model.load_weights(f'{model_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2540,
     "status": "ok",
     "timestamp": 1606204261758,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "-dgUfbe9qKOS",
    "outputId": "fe09be16-2a95-4a14-e202-60724a5fb396"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_source.imges, test_source.masks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InyTSG4Gqs8W"
   },
   "source": [
    "# **Predictions on training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "executionInfo": {
     "elapsed": 3153,
     "status": "ok",
     "timestamp": 1606204297619,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "9f8KwS7LqphC",
    "outputId": "c989d739-7bcd-41f8-df7f-f47388a0dada",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_func(image_in):\n",
    "    image_expanded = np.expand_dims(image_in, axis = 0)\n",
    "    return model.predict(image_expanded)\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)\n",
    "\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers import Activation, Concatenate, Add, ZeroPadding2D, BatchNormalization, Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, InputLayer, Conv2DTranspose, DepthwiseConv2D\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "lines_in_graph = []\n",
    "\n",
    "model_data_dir = os.path.join(model_dir, model_name)\n",
    "if not os.path.exists(model_data_dir):\n",
    "    os.makedirs(model_data_dir)\n",
    "    \n",
    "for i, layer in enumerate(model.layers): \n",
    "    print(i, \":\", layer.__class__.__name__, layer.name)\n",
    "    items_to_write = [layer.__class__.__name__, layer.name]\n",
    "    if isinstance(layer,InputLayer):\n",
    "        inpu_shape = layer.input.shape\n",
    "        items_to_write.append(\"\")\n",
    "        items_to_write.append(str(inpu_shape[2]))\n",
    "        items_to_write.append(str(inpu_shape[1]))\n",
    "        items_to_write.append(str(inpu_shape[3]))\n",
    "    elif isinstance(layer.input, list):\n",
    "        input_layer_names = []\n",
    "        for i in range(len(layer.input)):\n",
    "            inpu_layer_name = layer.input[i].name.split('/')[0]\n",
    "            input_layer_names.append(inpu_layer_name)\n",
    "        items_to_write.append(\"&\".join(input_layer_names))\n",
    "        if isinstance(layer,Add):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,Concatenate): \n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,HeadWrapper):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        else:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"Error: Multi input layer not known\")\n",
    "            print(\"---------------------------------\")            \n",
    "    else:\n",
    "        inpu_layer_name = layer.input.name.split('/')[0]\n",
    "        items_to_write.append(inpu_layer_name)\n",
    "        if isinstance(layer,Conv2D):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            layer_activation = layer.activation\n",
    "            if layer_activation:\n",
    "                items_to_write.append(layer_activation.__name__)\n",
    "            else:\n",
    "                print(\"Warning: Activation layer present : \", layer_activation.__name__)                \n",
    "                print(\"---------------------------------\")\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            dilation_rates = layer.dilation_rate\n",
    "            if dilation_rates[0] != dilation_rates[1] and dilation_rates[1] != 1:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: dilation_rates must be equal\")\n",
    "                print(\"---------------------------------\")                \n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "            items_to_write.append(str(layer.padding))            \n",
    "        elif isinstance(layer,Conv2DTranspose):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,DepthwiseConv2D):   \n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,BatchNormalization):\n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "            b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a) \n",
    "\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,MaxPooling2D):\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,ZeroPadding2D):\n",
    "            if(layer.padding[0][1] != layer.padding[1][1] and layer.padding[0][0] == 0 and layer.padding[1][0] != 0):\n",
    "                print(layer.padding)\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: This padding not supported\") \n",
    "                print(\"---------------------------------\")\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(layer.padding[0][0]))\n",
    "            items_to_write.append(str(layer.padding[0][1]))\n",
    "            items_to_write.append(str(layer.padding[1][0]))\n",
    "            items_to_write.append(str(layer.padding[1][1]))\n",
    "        elif isinstance(layer, ReLU):\n",
    "            maxValue = layer.max_value\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(maxValue))\n",
    "        elif isinstance(layer, Activation):\n",
    "            activation_type = layer.get_config()['activation']\n",
    "            maxValue = 0\n",
    "            if activation_type == 'relu':\n",
    "                items_to_write.append(\"relu\")\n",
    "                maxValue = 0\n",
    "                if hasattr(layer,'max_value'):\n",
    "                    maxValue = layer.max_value\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "                items_to_write.append(str(maxValue))\n",
    "            elif activation_type == 'sigmoid':\n",
    "                items_to_write.append(\"sigmoid\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "            elif activation_type == 'softmax':\n",
    "                items_to_write.append(\"softmax\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                output_shape = layer.output.shape\n",
    "                print(output_shape)\n",
    "                if len(output_shape) ==4: \n",
    "                    items_to_write.append(str(inpu_shape[2]))\n",
    "                    items_to_write.append(str(inpu_shape[1]))\n",
    "                    items_to_write.append(str(inpu_shape[3]))\n",
    "                elif len(output_shape) == 3: \n",
    "                    items_to_write.append(str(inpu_shape[1]))\n",
    "                    items_to_write.append(str(inpu_shape[2]))\n",
    "                else:\n",
    "                    print(\"---------------------------------\")\n",
    "                    print(\"Error: Unexpected shape \", activation_type)\n",
    "                    print(\"---------------------------------\") \n",
    "            else:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Only Relu activation with max value supported, provided is \", activation_type)\n",
    "                print(\"---------------------------------\") \n",
    "        else:\n",
    "            print(\"Error: Layer is not identified \", layer.__class__.__name__, layer.name)\n",
    "    lines_in_graph.append(\",\".join(items_to_write))\n",
    "with open(f'{model_name}.txt', 'w') as f:\n",
    "    # Write each line of the list to the file\n",
    "    for line in lines_in_graph:            \n",
    "        f.write(line + '\\n')\n",
    "    print(\"File is saved to \" + f'{model_name}.txt')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save the image in numpy format for testing in c++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_file_path = os.path.join(sample_test_dir, \"input.bmp\")\n",
    "selected_image = img_to_array(load_img(image_file_path).resize((w,h))).astype(np.uint8)\n",
    "\n",
    "img_data = preprocess_input(selected_image)\n",
    "\n",
    "numpy_in = os.path.join(sample_test_dir, \"in_mobilenet.npy\")\n",
    "print(numpy_in)\n",
    "np.save(numpy_in, np.transpose(img_data.copy().astype(np.float32),[1,0,2]))\n",
    "\n",
    "def pred_funct(image_in, model_p):\n",
    "    image_expanded = np.expand_dims(image_in, axis = 0)\n",
    "    return model_p.predict(image_expanded)\n",
    "\n",
    "out_img = pred_funct(img_data, model).squeeze()\n",
    "fig, (input_figure, output_figure1, output_figure2, output_figure3) = plt.subplots(1, 4, figsize = (20, 15))\n",
    "\n",
    "input_figure.imshow(selected_image)\n",
    "input_figure.set_title('Image')\n",
    "input_figure.set_axis_off()\n",
    "\n",
    "selected_mask = np.argmax(out_img, axis=2)\n",
    "\n",
    "input_figure.set_title('Image')\n",
    "input_figure.set_axis_off()\n",
    "output_figure1.imshow(selected_mask.squeeze(), cmap = 'gray')\n",
    "output_figure1.set_title('Mask Image1')\n",
    "output_figure1.set_axis_off()   \n",
    "output_figure2.imshow(selected_mask.squeeze() == 1, cmap = 'gray')\n",
    "output_figure2.set_title('Mask Image2')\n",
    "output_figure2.set_axis_off()   \n",
    "output_figure3.imshow(selected_mask.squeeze() == 2, cmap = 'gray')\n",
    "output_figure3.set_title('Mask Image3')\n",
    "output_figure3.set_axis_off()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model, check_index):\n",
    "    input_layer_data = model.layers[0]\n",
    "    intermediate_layer_data = model.layers[check_index-1]\n",
    "    output_layer_data = model.layers[check_index]\n",
    "\n",
    "    model1 = Model(input_layer_data.input, intermediate_layer_data.output)\n",
    "    model2 = Model(intermediate_layer_data.output, output_layer_data.output)\n",
    "\n",
    "    print(input_layer_data.name)\n",
    "    print(intermediate_layer_data.name)\n",
    "    print(output_layer_data.name)\n",
    "\n",
    "    print(input_layer_data.input.shape[1:])\n",
    "    print(intermediate_layer_data.output.shape[1:])\n",
    "    print(output_layer_data.output.shape[1:])\n",
    "    return model1, model2\n",
    "def write_csv3d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for l in range(output_data.shape[3]):\n",
    "            for k in range(output_data.shape[2]):\n",
    "                for j in range(output_data.shape[1]):\n",
    "                    for i in range(output_data.shape[0]):\n",
    "                        # Join the text data in each row with commas\n",
    "                        value = output_data[i, j, k, l]\n",
    "                        # Write the row data to the output file\n",
    "                        if i == output_data.shape[1] - 1:\n",
    "                            file.write(\"{:.4f}\".format(value))\n",
    "                        else:\n",
    "                            file.write(\"{:.4f},\".format(value))\n",
    "                    file.write(\"\\n\")\n",
    "            \n",
    "def write_csv(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for k in range(output_data.shape[2]):\n",
    "            for j in range(output_data.shape[0]):\n",
    "                for i in range(output_data.shape[1]):\n",
    "                    # Join the text data in each row with commas\n",
    "                    value = output_data[j, i, k]\n",
    "                    # Write the row data to the output file\n",
    "                    if i == output_data.shape[1] - 1:\n",
    "                        file.write(\"{:.4f}\".format(value))\n",
    "                    else:\n",
    "                        file.write(\"{:.4f},\".format(value))\n",
    "                file.write(\"\\n\")\n",
    "def runthis(mode1, model2, img_data, layer_name):\n",
    "    intermediate_output = pred_funct(img_data, model1).squeeze()\n",
    "    output = pred_funct(intermediate_output, model2).squeeze()\n",
    "    print(\"-----------------\")\n",
    "    for k in range(3):\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(img_data[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(img_data.shape)\n",
    "    print(\"-----------------\")\n",
    "    for k in range(min(3, intermediate_output.shape[2])): \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(intermediate_output[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(intermediate_output.shape)\n",
    "    write_csv(intermediate_output, layer_name +\"_in\")     \n",
    "    print(\"-----------------\")\n",
    "    for k in range(min(3, output.shape[2])): \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(output[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(output.shape)  \n",
    "    write_csv(output, layer_name + \"out\")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(model.layers))\n",
    "check_index = 1\n",
    "for index, layer in enumerate(model.layers):\n",
    "    #print(index, layer.name)\n",
    "    if layer.name in [\"activation_26\"]:\n",
    "        print(index, layer.name)\n",
    "        model1, model2 = generate_model(model, index)\n",
    "        runthis(model1, model2, img_data, layer.name)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"expanded_conv_project_BN\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_mean.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_variance.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "else:\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_bias.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    \n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_weights.npy')\n",
    "data = np.load(path_to_saved)\n",
    "if 'depth' in filer_name:\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 0, 0], end=\", \")\n",
    "        print()\n",
    "    print()\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 1, 0], end=\", \")\n",
    "        print()\n",
    "else:\n",
    "    for l in range(2):\n",
    "        for k in range(4):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    print(data[j, i, k ,l], end=\", \")\n",
    "                print()\n",
    "            print(\"\")\n",
    "        print(\"+++++++++\")\n",
    "    print(\"\\n--------------\")\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "VggUnet.ipynb",
   "provenance": [
    {
     "file_id": "1U-lQR00_5mk5jPArZSNTgqP0a8Vmlodz",
     "timestamp": 1606041309723
    },
    {
     "file_id": "https://github.com/ashishpatel26/Semantic-Segmentation-Keras-Tensorflow-Example/blob/main/Satellight_Image_Semantic_Segmentation_From_Scratch.ipynb",
     "timestamp": 1606031989735
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07ca0455627c47a78b5a99c64c285abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73d95f2e29c448bda8505e09581e1c0e",
      "placeholder": "",
      "style": "IPY_MODEL_c5c70e40ef4e4006b90c03d85f04ca97",
      "value": " 61/? [03:01&lt;00:00,  2.97s/it]"
     }
    },
    "19bd521b63ea4fbaaccb116fcd021a70": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "480ceb01acd74b91b57950f80b557e1e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dd4aa10b57e46ac9cb98bb92100c728": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f922376e800e4683b455f45b2eb9d9c8",
       "IPY_MODEL_07ca0455627c47a78b5a99c64c285abd"
      ],
      "layout": "IPY_MODEL_480ceb01acd74b91b57950f80b557e1e"
     }
    },
    "73d95f2e29c448bda8505e09581e1c0e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "951347637fa544d790335e75bd622fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c5c70e40ef4e4006b90c03d85f04ca97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f922376e800e4683b455f45b2eb9d9c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19bd521b63ea4fbaaccb116fcd021a70",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_951347637fa544d790335e75bd622fac",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
