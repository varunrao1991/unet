{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = \"A:/python_ai_projects/tool_tracking/tf-ssd\"\n",
    "model_name = \"ssd_test\"\n",
    "sample_test_dir = \"D:/python_ai_projects/tool_segmentation/sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae52b04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Input, InputLayer, BatchNormalization\n",
    "\n",
    "w = 10\n",
    "h = 10\n",
    "ch = 4\n",
    "stride = 1\n",
    "# Define a simple Keras model\n",
    "inputs = Input((h, w, ch), name=model_name)\n",
    "\n",
    "channels = 8\n",
    "x = Conv2D(channels,  (1, 1), padding='same', strides=(stride, stride), name=\"Conv1\")(inputs)\n",
    "output = BatchNormalization(name=\"bn\")(x)\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# Get the weights of the first layer\n",
    "layer1_weights = model.layers[1].get_weights()\n",
    "layer2_weights = model.layers[2].get_weights()\n",
    "\n",
    "# Create a new set of weights with the same shape as the original weights\n",
    "layer1_kernel_weight = np.random.uniform(low=0.0, high=2.0, size = layer1_weights[0].shape).astype(np.int32).astype(np.float32)\n",
    "layer1_bias = np.array([0 if x < 2 else 0 for x in range(channels)])\n",
    "new_layer1_weights = [layer1_kernel_weight, layer1_bias]\n",
    "print(new_layer1_weights[0].shape, new_layer1_weights[1].shape)\n",
    "\n",
    "gamma = np.ones(layer2_weights[0].shape)\n",
    "beta = np.zeros(layer2_weights[1].shape)\n",
    "moving_mean = np.zeros(layer2_weights[2].shape)\n",
    "moving_variance = np.ones(layer2_weights[3].shape)\n",
    "new_layer2_weights = [gamma, beta, moving_mean, moving_variance]\n",
    "\n",
    "if layer1_kernel_weight.shape[0] == layer1_kernel_weight.shape[1] and layer1_kernel_weight.shape[0] == 1 and len(layer1_kernel_weight.shape) == 4:\n",
    "    for i in range(min(7, layer1_kernel_weight.shape[3])):\n",
    "        for j in range(min(7, layer1_kernel_weight.shape[2])):     \n",
    "            print(layer1_kernel_weight[0,0,j,i], end=\", \")   \n",
    "        print(\"\\n\")\n",
    "else:       \n",
    "    for l in range(16):\n",
    "        for k in range(2):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if l > 2 or i != j:\n",
    "                        layer1_kernel_weight[i, j, k ,l] = 0\n",
    "                    else:\n",
    "                        layer1_kernel_weight[i, j, k ,l] = 1\n",
    "                    print(layer1_kernel_weight[j, i, k ,l], end=\", \")\n",
    "                print()\n",
    "            print()\n",
    "        print(\"---------\")\n",
    "    \n",
    "print(new_layer2_weights, len(new_layer2_weights))\n",
    "# Set the weights of the first layer to the new weights\n",
    "print(model.layers[1].name)\n",
    "print(new_layer1_weights)\n",
    "model.layers[1].set_weights(new_layer1_weights)\n",
    "print(model.layers[2].name)\n",
    "model.layers[2].set_weights(new_layer2_weights)\n",
    "\n",
    "\n",
    "def pred_funct(img_data, model_p):\n",
    "    img_in = np.expand_dims(img_data, axis = 0)\n",
    "    return model_p.predict(img_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6975b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Input, InputLayer, BatchNormalization, Conv2DTranspose\n",
    "\n",
    "\n",
    "w = 6\n",
    "h = 4\n",
    "ch = 16\n",
    "stride = 2\n",
    "# Define a simple Keras model\n",
    "inputs = Input((h, w, ch), name=model_name)\n",
    "\n",
    "channels = 16\n",
    "output = Conv2DTranspose(channels,  (2, 2), padding='same', strides=(stride, stride), use_bias = False, name=\"conv2d_transpose\")(inputs)\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# Get the weights of the first layer\n",
    "layer1_weights = model.layers[1].get_weights()\n",
    "print(\"Length : \", len(layer1_weights))\n",
    "print(\"Length : \", len(layer1_weights))\n",
    "\n",
    "# Create a new set of weights with the same shape as the original weights\n",
    "layer1_kernel_weight = np.random.uniform(low=0.0, high=2.0, size = layer1_weights[0].shape).astype(np.float32)\n",
    "new_layer1_weights = [layer1_kernel_weight]\n",
    "\n",
    "   \n",
    "# Set the weights of the first layer to the new weights\n",
    "print(model.layers[1].name)\n",
    "model.layers[1].set_weights(new_layer1_weights)\n",
    "\n",
    "\n",
    "def pred_funct(img_data, model_p):\n",
    "    img_in = np.expand_dims(img_data, axis = 0)\n",
    "    return model_p.predict(img_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1156c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.random.uniform(low=0.0, high=5.0, size = model.layers[0].input.shape[1:4]).astype(np.float32)\n",
    "numpy_in = os.path.join(sample_test_dir, \"in_\" + model_name + \".npy\")\n",
    "np.save(numpy_in, np.transpose(img_data,[1,0,2]))\n",
    "print(numpy_in)\n",
    "print(img_data.shape)\n",
    "\n",
    "for k in range(3):\n",
    "    for i in range(4):\n",
    "        for j in range(6):\n",
    "            print(img_data[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cabde8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = pred_funct(img_data, model).squeeze()\n",
    "print(img_data.shape)\n",
    "print(output.shape) \n",
    "print(\"-----------------\")\n",
    "for k in range(6):\n",
    "    for i in range(4):\n",
    "        for j in range(6):\n",
    "            print(img_data[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print(\"\\n\")\n",
    "print(\"-----------------\")\n",
    "for k in range(6): \n",
    "    for i in range(min(8,output.shape[0])):\n",
    "        for j in range(min(6,output.shape[1])):\n",
    "            print(output[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc367e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers import Activation, Concatenate, Add, ZeroPadding2D, BatchNormalization, Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, InputLayer, Conv2DTranspose, DepthwiseConv2D\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "lines_in_graph = []\n",
    "\n",
    "model_data_dir = os.path.join(model_dir, model_name)\n",
    "if not os.path.exists(model_data_dir):\n",
    "    os.makedirs(model_data_dir)\n",
    "    \n",
    "for i, layer in enumerate(model.layers): \n",
    "    print(i, \":\", layer.__class__.__name__, layer.name)\n",
    "    items_to_write = [layer.__class__.__name__, layer.name]\n",
    "    if isinstance(layer,InputLayer):\n",
    "        inpu_shape = layer.input.shape\n",
    "        items_to_write.append(\"\")\n",
    "        items_to_write.append(str(inpu_shape[2]))\n",
    "        items_to_write.append(str(inpu_shape[1]))\n",
    "        items_to_write.append(str(inpu_shape[3]))\n",
    "    elif isinstance(layer.input, list):\n",
    "        input_layer_names = []\n",
    "        for i in range(len(layer.input)):\n",
    "            inpu_layer_name = layer.input[i].name.split('/')[0]\n",
    "            input_layer_names.append(inpu_layer_name)\n",
    "        items_to_write.append(\"&\".join(input_layer_names))\n",
    "        if isinstance(layer,Add):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,Concatenate): \n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,HeadWrapper):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        else:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"Error: Multi input layer not known\")\n",
    "            print(\"---------------------------------\")            \n",
    "    else:\n",
    "        inpu_layer_name = layer.input.name.split('/')[0]\n",
    "        items_to_write.append(inpu_layer_name)\n",
    "        if isinstance(layer,Conv2D):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            dilation_rates = layer.dilation_rate\n",
    "            if dilation_rates[0] != dilation_rates[1] and dilation_rates[1] != 1:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: dilation_rates must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,Conv2DTranspose):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,DepthwiseConv2D):   \n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,BatchNormalization):\n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "            b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a) \n",
    "\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,MaxPooling2D):\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,ZeroPadding2D):\n",
    "            if(layer.padding[0][1] != layer.padding[1][1] and layer.padding[0][0] == 0 and layer.padding[1][0] != 0):\n",
    "                print(layer.padding)\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: This padding not supported\") \n",
    "                print(\"---------------------------------\")\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(layer.padding[0][0]))\n",
    "            items_to_write.append(str(layer.padding[0][1]))\n",
    "            items_to_write.append(str(layer.padding[1][0]))\n",
    "            items_to_write.append(str(layer.padding[1][1]))\n",
    "        elif isinstance(layer, ReLU):\n",
    "            maxValue = layer.max_value\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(maxValue))\n",
    "        elif isinstance(layer, Activation):\n",
    "            activation_type = layer.get_config()['activation']\n",
    "            maxValue = 0\n",
    "            if activation_type == 'relu':\n",
    "                items_to_write.append(\"relu\")\n",
    "                maxValue = 0\n",
    "                if hasattr(layer,'max_value'):\n",
    "                    maxValue = layer.max_value\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "                items_to_write.append(str(maxValue))\n",
    "            elif activation_type == 'sigmoid':\n",
    "                items_to_write.append(\"sigmoid\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "            elif activation_type == 'softmax':\n",
    "                items_to_write.append(\"softmax\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                output_shape = layer.output.shape\n",
    "                print(output_shape)\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "            else:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Only Relu activation with max value supported, provided is \", activation_type)\n",
    "                print(\"---------------------------------\") \n",
    "        else:\n",
    "            print(\"Error: Layer is not identified \", layer.__class__.__name__, layer.name)\n",
    "    lines_in_graph.append(\",\".join(items_to_write))\n",
    "with open(f'{model_dir}/{model_name}.txt', 'w') as f:\n",
    "    # Write each line of the list to the file\n",
    "    for line in lines_in_graph:            \n",
    "        f.write(line + '\\n')\n",
    "    print(\"File is saved to \" + f'{model_name}.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a72ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"Conv1\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_mean.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_variance.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "else:\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_bias.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    \n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_weights.npy')\n",
    "data = np.load(path_to_saved)\n",
    "\n",
    "for l in range(2):\n",
    "    for k in range(2):\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                print(data[j, i, k ,l], end=\", \")\n",
    "            print()\n",
    "        print(\"\")\n",
    "    print(\"+++++++++\")\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92afaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"block5_conv3\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}.npy')\n",
    "data = np.load(path_to_saved)\n",
    "\n",
    "print(data.shape)\n",
    "for l in range(2):\n",
    "    for k in range(16):\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                print(data[j, i, k ,l], end=\", \")\n",
    "            print()\n",
    "        print(\"\")\n",
    "    print(\"+++++++++\")\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1731507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(model.layers):\n",
    "    if \"activation_8\" == layer.name:\n",
    "        print(index)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_data = model.layers[0]\n",
    "intermediate_layer_data = model.layers[161]\n",
    "output_layer_data = model.layers[162]\n",
    "\n",
    "model1 = Model(input_layer_data.input, intermediate_layer_data.output)\n",
    "model2 = Model(intermediate_layer_data.output, output_layer_data.output)\n",
    "\n",
    "print(input_layer_data.name)\n",
    "print(intermediate_layer_data.name)\n",
    "print(output_layer_data.name)\n",
    "\n",
    "print(input_layer_data.input.shape[1:])\n",
    "print(intermediate_layer_data.output.shape[1:])\n",
    "print(output_layer_data.output.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = pred_funct(img_data, model1).squeeze()\n",
    "output = pred_funct(intermediate_output, model2).squeeze()\n",
    "print(\"-----------------\")\n",
    "for k in range(3):\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print(img_data[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print(\"\\n\")\n",
    "print(img_data.shape)\n",
    "print(\"-----------------\")\n",
    "for k in range(min(3, intermediate_output.shape[2])): \n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print(intermediate_output[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print(\"\\n\")\n",
    "print(intermediate_output.shape)  \n",
    "print(\"-----------------\")\n",
    "for k in range(min(3, output.shape[2])): \n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print(output[i, j, k], end=\", \")\n",
    "        print()\n",
    "    print(\"\\n\")\n",
    "print(output.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv3d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, output_layer_data.name + \"_\" + file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for l in range(output_data.shape[3]):\n",
    "            for k in range(output_data.shape[2]):\n",
    "                for j in range(output_data.shape[1]):\n",
    "                    for i in range(output_data.shape[0]):\n",
    "                        # Join the text data in each row with commas\n",
    "                        value = output_data[i, j, k, l]\n",
    "                        # Write the row data to the output file\n",
    "                        if i == output_data.shape[1] - 1:\n",
    "                            file.write(\"{:.4f}\".format(value))\n",
    "                        else:\n",
    "                            file.write(\"{:.4f},\".format(value))\n",
    "                    file.write(\"\\n\")\n",
    "                file.write(\"\\n\")\n",
    "            file.write(\"\\n\")   \n",
    "            \n",
    "def write_csv(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, output_layer_data.name + \"_\" + file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for k in range(output_data.shape[2]):\n",
    "            for j in range(output_data.shape[0]):\n",
    "                for i in range(output_data.shape[1]):\n",
    "                    # Join the text data in each row with commas\n",
    "                    value = output_data[j, i, k]\n",
    "                    # Write the row data to the output file\n",
    "                    if i == output_data.shape[1] - 1:\n",
    "                        file.write(\"{:.4f}\".format(value))\n",
    "                    else:\n",
    "                        file.write(\"{:.4f},\".format(value))\n",
    "                file.write(\"\\n\")\n",
    "            file.write(\"\\n\")       \n",
    "            \n",
    "write_csv(output, \"out\")\n",
    "write_csv(intermediate_output, \"in\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b61302",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.random.uniform(low=0.0, high=10.0, size = input_layer_data.input.shape[1:]).astype(np.int32).astype(np.float32)\n",
    "#img_data = np.ones(input_layer_data.input.shape[1:], dtype=np.float32)\n",
    "#img_data = np.zeros(input_layer_data.input.shape[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"Conv1\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_mean.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_variance.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "else:\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_bias.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(5):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    \n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_weights.npy')\n",
    "data = np.load(path_to_saved)\n",
    "if 'depth' in filer_name:\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 0, 0], end=\", \")\n",
    "        print()\n",
    "    print()\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 1, 0], end=\", \")\n",
    "        print()\n",
    "else:\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[0, 0, i, j], end=\", \")\n",
    "        print()\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ab273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filer_name = \"expanded_conv_project\"\n",
    "filer_name = \"Conv1\"\n",
    "numpy_in = os.path.join(sample_test_dir, filer_name + \"_in.npy\")\n",
    "np.save(numpy_in, np.transpose(img_data,[1,0,2]))\n",
    "print(numpy_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"conv2d_transpose\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{file_name}.npy')\n",
    "data = np.load(path_to_saved)\n",
    "\n",
    "for l in range(2):\n",
    "    for k in range(2):\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                print(data[j, i, k ,l], end=\", \")\n",
    "            print()\n",
    "        print(\"\")\n",
    "    print(\"+++++++++\")\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")\n",
    "\n",
    "write_csv3d(data, \"filter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
