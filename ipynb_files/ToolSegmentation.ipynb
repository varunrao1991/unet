{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1606203804009,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "PLNZ8zakUs6W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1439,
     "status": "ok",
     "timestamp": 1606203670303,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "yGFDYtaqByBF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb2bgr(input_img):\n",
    "    bgr_image = np.zeros_like(input_img)\n",
    "    bgr_image[:,:,0] = input_img[:,:,2] # R channel to B channel\n",
    "    bgr_image[:,:,1] = input_img[:,:,1] # G channel remains the same\n",
    "    bgr_image[:,:,2] = input_img[:,:,0] # B channel to R channel\n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4190,
     "status": "ok",
     "timestamp": 1606203813346,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "SnRR09qRUv9S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "import random \n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiODc8j9cPj6"
   },
   "source": [
    "# **Directory reference, data augmentor, data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2261,
     "status": "ok",
     "timestamp": 1606203813355,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "xZthNfffVUY6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, h = 320,240\n",
    "base_data_dir = \"A:/data/demo_video_data\"\n",
    "model_dir = \"A:/models/tool_segmentation\"\n",
    "sample_test_dir = \"A:/python_ai_projects/tool_segmentation/sample_data\"\n",
    "image_directory_name = \"foreground\"\n",
    "mask_directory_name = \"masks\"\n",
    "background_dir_name = \"background\"\n",
    "model_name = \"scoop_segment\"\n",
    "\n",
    "no_bg_change_list_file = os.path.join(base_data_dir, 'no_background_change.txt')\n",
    "lines = []\n",
    "if os.path.exists(no_bg_change_list_file):\n",
    "    with open(no_bg_change_list_file, 'r') as f:\n",
    "        # Read the contents of the file into a list of strings\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Strip the newline character from each line\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "\n",
    "file_names_to_exclude_from_bg_change = lines\n",
    "print(file_names_to_exclude_from_bg_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "def activator_labels(images, augmenter, parents, default):\n",
    "    if augmenter.name in [\"AddToHueAndSaturation\", \"GaussianBlur\"]: # \"noaugment\"\n",
    "        return False\n",
    "    else:\n",
    "        return default\n",
    "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "            [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.GaussianBlur(sigma=(0,0.05), name= \"GaussianBlur\"),\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "            # crop images by -5% to 10% of their height/width\n",
    "            sometimes(iaa.CropAndPad(\n",
    "                percent=(-0.005, 0.001),\n",
    "                pad_mode=ia.ALL,\n",
    "                pad_cval=(0, 0)\n",
    "            )),\n",
    "            sometimes(iaa.Affine(\n",
    "                scale=(0.9, 1.005), # scale images to 90-110% of their size, individually per axis\n",
    "                translate_percent={\"x\": (-0.01, 0.01), \"y\": (-0.01, 0.01)}, # translate by -20 to +20 percent (per axis)\n",
    "\n",
    "                rotate=(-2, 2), # rotate by -5 to +5 degrees\n",
    "                #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                #cval=(0, 0), # if mode is constant, use a cval between 0 and 255\n",
    "                #mode=ia.ALL, # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "                shear=(-0.5, 0.5) # shear by -16 to +16 degrees\n",
    "            )),\n",
    "            iaa.Scale(0.25),\n",
    "            iaa.CropToFixedSize(w,h),\n",
    "            iaa.AddToHueAndSaturation((-20, 20), name= \"AddToHueAndSaturation\"), # change the hue and saturation by up to 20\n",
    "            ],\n",
    "            random_order=False\n",
    "        )\n",
    "hooks_labels = ia.HooksImages(activator=activator_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2552,
     "status": "ok",
     "timestamp": 1606203817958,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "My7rnnciXkJH",
    "outputId": "dec61c8d-e53c-431e-8681-e3c65d3e0047"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "imag_paths = sorted(glob.glob(os.path.join(base_data_dir, image_directory_name + \"/*.jpg\"), recursive=True))\n",
    "mask_paths = sorted(glob.glob(os.path.join(base_data_dir, mask_directory_name + \"/*.png\"), recursive=True))\n",
    "bkgd_paths = sorted(glob.glob(os.path.join(base_data_dir, background_dir_name + \"/*.jpg\"), recursive=True))\n",
    "\n",
    "print(f'Total Train Images : {len(imag_paths)}')\n",
    "print(f'Total Mask Image : {len(mask_paths)}')\n",
    "print(f'Total background images : {len(bkgd_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1606204000629,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "c8HsqwowWnFz",
    "outputId": "cbb35091-5b2c-48b9-8576-32072a75b237"
   },
   "outputs": [],
   "source": [
    "# Split train and valid\n",
    "train_imag_paths, test_imag_paths, train_mask_paths, test_mask_paths = train_test_split(imag_paths, mask_paths, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6dd4aa10b57e46ac9cb98bb92100c728",
      "480ceb01acd74b91b57950f80b557e1e",
      "f922376e800e4683b455f45b2eb9d9c8",
      "07ca0455627c47a78b5a99c64c285abd",
      "951347637fa544d790335e75bd622fac",
      "19bd521b63ea4fbaaccb116fcd021a70",
      "c5c70e40ef4e4006b90c03d85f04ca97",
      "73d95f2e29c448bda8505e09581e1c0e"
     ]
    },
    "executionInfo": {
     "elapsed": 176823,
     "status": "ok",
     "timestamp": 1606203994349,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "uHmA_NWTWV0S",
    "outputId": "dda2b3c6-3bb3-423b-9d63-b3090a42c0a5"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 2\n",
    "class ImageSource:\n",
    "    def __init__(self, imag_paths, mask_paths, bkgd_paths, background_aug = False):\n",
    "        self.imges = np.zeros((len(imag_paths), h, w, 3), dtype=np.float32)\n",
    "        self.masks = np.zeros((len(mask_paths), h, w, num_classes), dtype=np.float32)\n",
    "        \n",
    "        self.imag_paths = imag_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.bkgd_paths = bkgd_paths\n",
    "        \n",
    "        self.total_images = len(imag_paths)\n",
    "\n",
    "        self.max_blur_size = 10\n",
    "        self.min_blur_size = 1\n",
    "        self.max_circle_size = 0.11\n",
    "        \n",
    "        self.rgb_value = [0.485, 0.456, 0.406]\n",
    "        self.background_aug = background_aug\n",
    "        self.find_class_weights()\n",
    "        self.generate_augmented_images()\n",
    "\n",
    "    def generate_augmented_images(self):\n",
    "        class_weights = [0 for i in range(num_classes)]\n",
    "        for n, (image_path, mask_path) in tqdm(enumerate(zip(self.imag_paths, self.mask_paths))):\n",
    "            #print(os.path.split(image_path)[1], os.path.split(mask_path)[1])\n",
    "            random_background_image_index = random.randint(0, len(bkgd_paths) - 1)\n",
    "\n",
    "            imag_data = img_to_array(load_img(image_path)).astype(np.uint8)\n",
    "            mask_data = img_to_array(load_img(mask_path, color_mode = \"grayscale\")).astype(np.uint8).squeeze()\n",
    "\n",
    "            seq_deterministic = seq.to_deterministic() \n",
    "            imag_data_augmented = seq_deterministic.augment_image(imag_data)\n",
    "            mask_data_augmented_binary = seq_deterministic.augment_image(mask_data, hooks=hooks_labels) > 0\n",
    "\n",
    "            image_filename = os.path.split(image_path)[1]\n",
    "            exclude_file = image_filename in file_names_to_exclude_from_bg_change\n",
    "            #print(image_filename, exclude_file)\n",
    "            has_mask = mask_data.max() > 0\n",
    "            if self.background_aug and has_mask and not exclude_file:\n",
    "                bkgd_path = self.bkgd_paths[random_background_image_index]\n",
    "                bkgd_data = img_to_array(load_img(bkgd_path)).astype(np.uint8)\n",
    "                bkgd_data_augmented = seq_deterministic.augment_image(bkgd_data)\n",
    "                expanded_mask = np.expand_dims(mask_data_augmented_binary, -1)\n",
    "                mask = np.repeat(expanded_mask, 3, axis=-1)\n",
    "                imag_data_augmented[~mask] = bkgd_data_augmented[~mask]\n",
    "            image_blended_float = imag_data_augmented.astype(np.float32)\n",
    "\n",
    "            self.imges[n] = preprocess_input(image_blended_float)/255.0\n",
    "            mask_3d = to_categorical(mask_data_augmented_binary, num_classes)\n",
    "            self.masks[n] = mask_3d.astype(np.float32)\n",
    "\n",
    "    def find_class_weights(self):\n",
    "        class_weights = [0 for i in range(num_classes)]\n",
    "        for n, mask_path in tqdm(enumerate(self.mask_paths)):\n",
    "            mask_data = img_to_array(load_img(mask_path, color_mode = \"grayscale\")).astype(np.uint8).squeeze() > 0\n",
    "            total_pixels = mask_data.shape[0] * mask_data.shape[1]\n",
    "            for i in range(num_classes):\n",
    "                class_weight = np.count_nonzero(mask_data == i) / total_pixels\n",
    "                class_weights[i] += class_weight\n",
    "                \n",
    "        self.class_weights = [1 - (class_weights[i] / self.total_images) for i in range(num_classes)]\n",
    "        print(self.class_weights)\n",
    "\n",
    "    def display_images(self):\n",
    "        # Visualize any randome image along with the mask\n",
    "        random_index = random.randint(0, len(self.imges)-1)\n",
    "        selected_image = self.imges[random_index]\n",
    "        selected_mask = np.argmax(self.masks[random_index], axis=2)\n",
    "        has_mask = selected_image.max() > 0\n",
    "\n",
    "        fig, (input_figure, output_figure) = plt.subplots(1, 2, figsize = (20, 15))\n",
    "        reverted_preprocessed_image = selected_image + self.rgb_value\n",
    "\n",
    "        input_figure.imshow(rgb2bgr(reverted_preprocessed_image))\n",
    "        if has_mask: # if salt\n",
    "            # draw a boundary(contour) in the original image separating salt and non-salt areas\n",
    "            input_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "\n",
    "        input_figure.set_title('Image')\n",
    "        input_figure.set_axis_off()\n",
    "\n",
    "        output_figure.imshow(selected_mask.squeeze(), cmap = 'gray')\n",
    "        output_figure.set_title('Mask Image')\n",
    "        output_figure.set_axis_off()       \n",
    "        \n",
    "\n",
    "    def display_images_pred(self, predict_func):\n",
    "        # Visualize any randome image along with the mask\n",
    "        random_index = random.randint(0, len(self.imges)-1)\n",
    "        selected_image = self.imges[random_index]\n",
    "        selected_mask = np.argmax(self.masks[random_index], axis=-1)\n",
    "        has_mask = selected_image.max() > 0\n",
    "\n",
    "        fig, (input_figure, output_figure, pred_figure) = plt.subplots(1, 3, figsize = (20, 15))\n",
    "        reverted_preprocessed_image = selected_image + self.rgb_value\n",
    "\n",
    "        input_figure.imshow(rgb2bgr(reverted_preprocessed_image))\n",
    "        if has_mask:\n",
    "            input_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "\n",
    "        input_figure.set_title('Image')\n",
    "        input_figure.set_axis_off()\n",
    "\n",
    "        output_figure.imshow(selected_mask.squeeze(), cmap = 'gray')\n",
    "        if has_mask:\n",
    "            output_figure.contour(selected_mask.squeeze(), colors = 'k', linewidths = 5, levels = [0.25])\n",
    "        output_figure.set_title('Mask Image')\n",
    "        output_figure.set_axis_off()   \n",
    "\n",
    "        pred = np.argmax(predict_func(selected_image), axis=-1)\n",
    "        pred_figure.imshow(pred.squeeze(), cmap = 'gray')\n",
    "        pred_figure.set_title('Predicted Image')\n",
    "        pred_figure.set_axis_off()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcYO1QSucdMy"
   },
   "source": [
    "# **Instantiate validation and training sources; visualize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "executionInfo": {
     "elapsed": 3742,
     "status": "ok",
     "timestamp": 1606204006196,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "9zTCOdJvcF1C",
    "outputId": "d204f2af-990a-4868-922a-59e965177ca5"
   },
   "outputs": [],
   "source": [
    "train_source = ImageSource(train_imag_paths, train_mask_paths, bkgd_paths, background_aug = True)\n",
    "test_source = ImageSource(test_imag_paths, test_mask_paths, bkgd_paths, background_aug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_source.imges.shape)\n",
    "print(train_source.masks.shape)\n",
    "train_source.display_images()\n",
    "test_source.display_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNET model function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "def UnetModel(image_width, image_height, vgg_weight_path=None, filter_size = 64, num_classes = 2):\n",
    "    inputs = Input((image_height, image_width, 3), name=\"vgg\")\n",
    "    # Block 1\n",
    "    x = Conv2D(filter_size, (3, 3), padding='same', name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filter_size, (3, 3), padding='same', name='block1_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_1_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_1_out)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(2 * filter_size, (3, 3), padding='same', name='block2_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(2 * filter_size, (3, 3), padding='same', name='block2_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_2_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_2_out)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(4 * filter_size, (3, 3), padding='same', name='block3_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(4 * filter_size, (3, 3), padding='same', name='block3_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(4 * filter_size, (3, 3), padding='same', name='block3_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_3_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_3_out)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block4_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block4_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block4_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_4_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_4_out)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block5_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block5_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(8 * filter_size, (3, 3), padding='same', name='block5_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    for_pretrained_weight = MaxPooling2D()(x)\n",
    "\n",
    "    # Load pretrained weights.\n",
    "    if vgg_weight_path is not None:\n",
    "        if filter_size == 64:\n",
    "            vgg16 = Model(inputs, for_pretrained_weight)\n",
    "            vgg16.load_weights(vgg_weight_path, by_name=True)\n",
    "            for layer in vgg16.layers:\n",
    "                layer.trainable = False\n",
    "        else:\n",
    "            print(\"Cannot load vgg model because filter size is not 64\")\n",
    "    # UP 1\n",
    "    x = Conv2DTranspose(8 * filter_size, kernel_size=(2,2), strides=(2,2), padding='same', use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, block_4_out])\n",
    "    x = Conv2D(4 * filter_size, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 2\n",
    "    x = Conv2DTranspose(4 * filter_size, kernel_size=(2,2), strides=(2,2), padding='same', use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, block_3_out])\n",
    "    x = Conv2D(2 * filter_size, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 3\n",
    "    x = Conv2DTranspose(2 * filter_size, kernel_size=(2,2), strides=(2,2), padding='same', use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, block_2_out])\n",
    "    x = Conv2D(filter_size, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 4\n",
    "    x = Conv2DTranspose(filter_size, kernel_size=(2,2), strides=(2,2), padding='same', use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, block_1_out])\n",
    "\n",
    "    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Activation('sigmoid')(x)\n",
    "    \n",
    "    print(f'Output of mode has {output_layer.shape} shape')\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetModel(w, h, \"../../models/vgg16_weights_tf_dim_ordering_tf_kernels.h5\", 16)\n",
    "#model = UnetModelSmall(w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss function and training parameters; Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1606204194502,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "v6vsC6vVjwe3",
    "outputId": "b4b0fe8a-a6a3-4ba7-c9f2-1e65e22e94ea"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "EPOCHES = 500\n",
    "RELOAD_ON_EVERY = 20\n",
    "EPOCH_PATIENCE = 25\n",
    "LEARNING_RATE_PATIENCE = 5\n",
    "LERNING_RATE = 0.01\n",
    "\n",
    "K.clear_session()\n",
    "class_weights = train_source.class_weights\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    # convert class weights dictionary to tensor\n",
    "    class_weights_tensor = K.constant(class_weights)\n",
    "    \n",
    "    # calculate the cross-entropy loss for each sample\n",
    "    per_sample_loss = categorical_crossentropy(y_true * class_weights_tensor, y_pred * class_weights_tensor)\n",
    "    # calculate the weighted loss for each sample\n",
    "    weighted_loss = K.sum(per_sample_loss)\n",
    "    \n",
    "    return weighted_loss\n",
    "\n",
    "class_weights = train_source.class_weights\n",
    "\n",
    "metrics = [\"accuracy\", tf.keras.metrics.AUC()]\n",
    "model.compile(optimizer=Adam(learning_rate=LERNING_RATE), loss=weighted_categorical_crossentropy, metrics=metrics)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "               \n",
    "class UpdateDataSample(Callback):\n",
    "    def __init__(self, train_data_source, batch_size = 4, update_frequency=10):\n",
    "        super(UpdateDataSample, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data_source = train_data_source\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch + 1) % self.update_frequency == 0:\n",
    "            self.train_data_source.generate_augmented_images()\n",
    "            num_batches = self.train_data_source.total_images // self.batch_size\n",
    "            for i in range(num_batches):\n",
    "                batch_x = self.train_data_source.imges[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                batch_y = self.train_data_source.masks[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                self.model.train_on_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=EPOCH_PATIENCE, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LEARNING_RATE_PATIENCE, min_lr=0.0001, verbose=1),\n",
    "    ModelCheckpoint(f'{model_name}.h5', \n",
    "                    verbose=1, \n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True),\n",
    "    CSVLogger(f\"{model_name}.csv\"),\n",
    "    TensorBoard(log_dir=f'./{model_name}_logs'),\n",
    "    UpdateDataSample(train_source, BATCH_SIZE, RELOAD_ON_EVERY)\n",
    "]\n",
    "\n",
    "#if os.path.exists(f'{model_name}.h5'):\n",
    "#    model.load_weights(f'{model_name}.h5')\n",
    "\n",
    "results = model.fit(train_source.imges, train_source.masks, batch_size = BATCH_SIZE, epochs=EPOCHES, callbacks=callbacks, validation_data=(test_source.imges, test_source.masks), use_multiprocessing=True)\n",
    "\n",
    "df_result = pd.DataFrame(results.history)\n",
    "df_result.sort_values('val_loss', ascending=True, inplace = True)\n",
    "df_result\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.plot(results.history[\"val_accuracy\"], label=\"val_Accuracy\")\n",
    "plt.plot(np.argmax(results.history[\"val_accuracy\"]), np.max(results.history[\"val_accuracy\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoTtCHM-qEBs"
   },
   "source": [
    "# **Inference; Dump model to use in c++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1606204258492,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "w3T22zhYpuaW"
   },
   "outputs": [],
   "source": [
    "model.load_weights(f'{model_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2540,
     "status": "ok",
     "timestamp": 1606204261758,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "-dgUfbe9qKOS",
    "outputId": "fe09be16-2a95-4a14-e202-60724a5fb396"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_source.imges, test_source.masks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = w\n",
    "init_h = h\n",
    "init_c = 3\n",
    "tap_count = 0\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "model_data_dir = os.path.join(model_dir, model_name)\n",
    "if not os.path.exists(model_data_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(model_data_dir)\n",
    "\n",
    "maxpool_count = 0\n",
    "\n",
    "layers_map = []\n",
    "tap_list = []\n",
    "\n",
    "for i, layer in enumerate(model.layers): \n",
    "    #print(i, layer.name)\n",
    "    #print(\"----\", layer)\n",
    "    if isinstance(layer, ReLU):\n",
    "        maxValue = layer.max_value\n",
    "        layers_map.append((f'relu,{init_w},{init_h},{init_c},{maxValue},{layer.name}', layer.name))  \n",
    "    elif isinstance(layer, Activation):\n",
    "        activation_type = layer.get_config()['activation']\n",
    "        if activation_type == 'relu':\n",
    "            maxValue = 0\n",
    "            if hasattr(layer,'max_value'):\n",
    "                maxValue = layer.max_value\n",
    "            layers_map.append((f'{activation_type},{init_w},{init_h},{init_c},{maxValue},{layer.name}', layer.name))  \n",
    "        else:\n",
    "            layers_map.append((f'{activation_type},{init_w},{init_h},{init_c},{layer.name}', layer.name)) \n",
    "    elif isinstance(layer, BatchNormalization):\n",
    "        print(i, layer.name)\n",
    "        if i == 0:\n",
    "            print(\"batchnormalization, 0th layer\")\n",
    "        elif isinstance(model.layers[i - 1], Conv2DTranspose):\n",
    "            layers_map.append((f'batchnormalization,{init_w},{init_h},{init_c},{layer.name}', layer.name))  \n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            if model.layers[i - 1].use_bias:\n",
    "                print(\"batchnormalization, i-1th layer transposed conv2d with bias\")\n",
    "                bias = model.layers[i - 1].weights[1]\n",
    "                #print(bias.shape, moving_mean.shape, moving_variance.shape, beta.shape, gamma.shape)\n",
    "                a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                b = (bias - moving_mean) * a + beta\n",
    "            else:\n",
    "                print(\"batchnormalization, i-1th layer transposed conv2d\")\n",
    "                a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a)    \n",
    "        elif isinstance(model.layers[i - 1], Conv2D) or isinstance(model.layers[i - 1], DepthwiseConv2D):\n",
    "            print(\"batchnormalization, i-1th layer conv2d or depthwiseconv2d\")\n",
    "            continue  \n",
    "        else:\n",
    "            print(\"batchnormalization layer is independent\")\n",
    "            layers_map.append((f'batchnormalization,{init_w},{init_h},{init_c},{layer.name}', layer.name))  \n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "            b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a)\n",
    "\n",
    "    elif isinstance(layer, MaxPooling2D):  \n",
    "        layers_map.append((f'maxpool,{init_w},{init_h},{init_c},maxpool_{maxpool_count}', layer.name))\n",
    "        init_w = int(init_w /2)\n",
    "        init_h = int(init_h /2)\n",
    "        maxpool_count += 1\n",
    "    elif isinstance(layer, Conv2DTranspose):  \n",
    "        if layer.use_bias and i + 1 < len(model.layers) and not isinstance(model.layers[i + 1], BatchNormalization):\n",
    "            print(\"Not implemeneted transposed convolution with bias with next layer not BatchNormalization\")\n",
    "            continue\n",
    "        filter_weights = layer.weights[0]      \n",
    "        conv_weight_shape = filter_weights.shape\n",
    "        layers_map.append((f'deconv,{conv_weight_shape[0]},{conv_weight_shape[1]},{conv_weight_shape[2]},{conv_weight_shape[3]},{init_w},{init_h},{init_c},{layer.name}', layer.name))\n",
    "        init_c = conv_weight_shape[3]\n",
    "        init_w = init_w *2\n",
    "        init_h = init_h *2\n",
    "        path_to_save = os.path.join(model_data_dir, layer.name + '.npy')\n",
    "        np.save(path_to_save, filter_weights)\n",
    "        if layer.use_bias:\n",
    "            bias = layer.weights[1]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "            np.save(path_to_save, bias)\n",
    "    elif isinstance(layer, Conv2D):\n",
    "        if i + 1 < len(model.layers) and isinstance(model.layers[i + 1], BatchNormalization):\n",
    "            nextLayer = model.layers[i + 1]\n",
    "            conv_weight_shape = layer.weights[0].shape\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"Strides must be equal\")\n",
    "                continue\n",
    "            if conv_weight_shape[0] == 1 and conv_weight_shape[1] == 1:\n",
    "                if strides[0] != 1:\n",
    "                    print(\"Only stride of 1 is supported in pointwise\")\n",
    "                    continue\n",
    "                else:\n",
    "                    layers_map.append((f'pointwise,{conv_weight_shape[2]},{conv_weight_shape[3]},{init_w},{init_h},{init_c},{layer.name}', nextLayer.name))\n",
    "            else:\n",
    "                if strides[0] != 1 and strides[0] != 2:\n",
    "                    print(f\"Only stride of 1 or 2 is supported in convolution {strides[0]}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    layers_map.append((f'conv,{conv_weight_shape[0]},{conv_weight_shape[1]},{conv_weight_shape[2]},{conv_weight_shape[3]},{init_w},{init_h},{init_c},{strides[0]},{layer.name}', nextLayer.name))    \n",
    "            init_c = conv_weight_shape[3] \n",
    "            init_w = int(init_w / strides[0])\n",
    "            init_h = int(init_h / strides[0])\n",
    "        \n",
    "            weights = nextLayer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                #print(bias.shape, moving_mean.shape, moving_variance.shape, beta.shape, gamma.shape)\n",
    "                a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                b = (bias - moving_mean) * a + beta\n",
    "            else:\n",
    "                a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, layer.weights[0])\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a)\n",
    "        else:\n",
    "            print(f\"Conv2D without batch normalization not implemented {layer.name}\")\n",
    "    elif isinstance(layer, DepthwiseConv2D):\n",
    "        strides = layer.strides\n",
    "        if strides[0] != strides[1]:\n",
    "            print(\"Strides must be equal\")\n",
    "            continue\n",
    "        if strides[0] != 1 and strides[0] != 2:\n",
    "            print(f\"Only stride of 1 or 2 and  equal size is supported in depthwise convolution {strides[0]}\")\n",
    "            continue\n",
    "        else:        \n",
    "            if i + 1 < len(model.layers) and isinstance(model.layers[i + 1], BatchNormalization):\n",
    "                conv_weight_shape = layer.weights[0].shape\n",
    "                if conv_weight_shape[3] != 1:\n",
    "                    print(\"Only 3D shape supported for weights in depthwise\")\n",
    "                nextLayer = model.layers[i + 1]\n",
    "                layers_map.append((f'depthwise,{conv_weight_shape[0]},{conv_weight_shape[1]},{conv_weight_shape[2]},{init_w},{init_h},{init_c},{strides[0]},{layer.name}', nextLayer.name))\n",
    "\n",
    "                weights = nextLayer.get_weights()\n",
    "                gamma = weights[0]\n",
    "                beta = weights[1]\n",
    "                moving_mean = weights[2]\n",
    "                moving_variance = weights[3]\n",
    "                if layer.use_bias:\n",
    "                    bias = layer.weights[1]\n",
    "                    #print(bias.shape, moving_mean.shape, moving_variance.shape, beta.shape, gamma.shape)\n",
    "                    a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                    b = (bias - moving_mean) * a + beta\n",
    "                else:\n",
    "                    a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "                    b = - moving_mean * a + beta\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "                np.save(path_to_save, layer.weights[0])\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "                np.save(path_to_save, b)\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "                np.save(path_to_save, a)\n",
    "            else:\n",
    "                conv_weight_shape = layer.weights[0].shape\n",
    "                if conv_weight_shape[3] != 1:\n",
    "                    print(\"Only 3D shape supported for weights in depthwise\")\n",
    "                nextLayer = model.layers[i + 1]                \n",
    "                if strides[0] != 1:\n",
    "                    print(\"Only stride of 1 is supported in depthwisebias\")\n",
    "                    continue\n",
    "                if layer.use_bias:\n",
    "                    layers_map.append((f'depthwisebias,{conv_weight_shape[0]},{conv_weight_shape[1]},{conv_weight_shape[2]},{init_w},{init_h},{init_c},{layer.name}', layer.name))\n",
    "                    bias = layer.weights[1]\n",
    "                    path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                    np.save(path_to_save, bias)\n",
    "                else:\n",
    "                    print(\"Make sure that depthwisenobias i implemented\")\n",
    "                    layers_map.append((f'depthwisenobias,{conv_weight_shape[0]},{conv_weight_shape[1]},{conv_weight_shape[2]},{init_w},{init_h},{init_c},{layer.name}', layer.name))\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "                np.save(path_to_save, layer.weights[0])\n",
    "                \n",
    "        init_w = int(init_w / strides[0])\n",
    "        init_h = int(init_h / strides[0])\n",
    "\n",
    "    elif isinstance(layer, InputLayer):\n",
    "        print(\"Input layer is\", layer.name)\n",
    "        layers_map.append((f'input,{init_w},{init_h},{init_c},{layer.name}',layer.name))\n",
    "        continue\n",
    "    elif isinstance(layer, Concatenate):\n",
    "        in1 = layer._inbound_nodes[0].input_tensors[0].shape\n",
    "        in2 = layer._inbound_nodes[0].input_tensors[1].shape\n",
    "        in2_name = layer._inbound_nodes[0].input_tensors[1].name.split('/')[0]\n",
    "        layers_map.append((f'concatenate,{tap_count},{in1[2]},{in1[1]},{in1[3]},{in2[2]},{in2[1]},{in2[3]},{layer.name}',layer.name))\n",
    "        tap_list.append((tap_count, in2_name))\n",
    "        init_c = in1[3] + in2[3]\n",
    "        tap_count += 1\n",
    "    elif isinstance(layer, ZeroPadding2D):\n",
    "        if(layer.padding[0][1] != layer.padding[1][1] and layer.padding[0][0] == 0 and layer.padding[1][0] != 0):\n",
    "            print(layer.padding)\n",
    "            print(\"This padding not supported\")\n",
    "            continue\n",
    "        layers_map.append((f'padding,{init_w},{init_h},{init_c},{layer.padding[0][1]},{layer.name}',layer.name))\n",
    "        init_w = init_w + layer.padding[0][1]\n",
    "        init_h = init_h + layer.padding[0][1]\n",
    "    elif isinstance(layer, Add):\n",
    "        in1 = layer._inbound_nodes[0].input_tensors[0].shape\n",
    "        in2 = layer._inbound_nodes[0].input_tensors[1].shape\n",
    "        in1_name = layer._inbound_nodes[0].input_tensors[0].name.split('/')[0]\n",
    "        layers_map.append((f'add,{tap_count},{in1[2]},{in1[1]},{in1[3]},{in2[2]},{in2[1]},{in2[3]},{layer.name}',layer.name))\n",
    "        tap_list.append((tap_count, in1_name))\n",
    "        tap_count += 1\n",
    "    else:\n",
    "        print(f'Unhandled layer {layer}')\n",
    "\n",
    "for tap in tap_list:\n",
    "    temp_layer_map = list(layers_map)\n",
    "    temp_layer_map_len = len(temp_layer_map)\n",
    "    for index, layer in enumerate(temp_layer_map):\n",
    "        if tap[1] == layer[-1]:\n",
    "            layers_map.insert(index+1, (f'tap,{tap[0]}', tap[1]))\n",
    "            break\n",
    "        if temp_layer_map_len-1 == index:\n",
    "            print(f'Not found {tap[1]}')\n",
    "\n",
    "with open(f'{model_name}.txt', 'w') as f:\n",
    "    # Write each line of the list to the file\n",
    "    for layer in layers_map:            \n",
    "        f.write(layer[0] + '\\n')\n",
    "    print(\"File is saved to \" + f'{model_name}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InyTSG4Gqs8W"
   },
   "source": [
    "# **Predictions on training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "executionInfo": {
     "elapsed": 3153,
     "status": "ok",
     "timestamp": 1606204297619,
     "user": {
      "displayName": "Best Proctor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAVkilScIH8xC6hu8VFLIWwO_fJAz62pawFI4w=s64",
      "userId": "14482452209629887617"
     },
     "user_tz": -330
    },
    "id": "9f8KwS7LqphC",
    "outputId": "c989d739-7bcd-41f8-df7f-f47388a0dada",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_func(image_in):\n",
    "    image_expanded = np.expand_dims(image_in, axis = 0)\n",
    "    return model.predict(image_expanded)\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "test_source.display_images_pred(pred_func)\n",
    "\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)\n",
    "train_source.display_images_pred(pred_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save the image in numpy format for testing in c++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_file_path = os.path.join(sample_test_dir, \"09-45-20_WHITELIGHT-Left_02-Mar-2023_frame_300.jpg\")\n",
    "selected_image = img_to_array(load_img(image_file_path).resize((w,h))).astype(np.uint8)\n",
    "\n",
    "selected_image_f = selected_image.astype(np.float32) / 255.0\n",
    "selected_image_f -= [0.485, 0.456, 0.406]\n",
    "img_data = rgb2bgr(selected_image_f)\n",
    "\n",
    "\n",
    "def pred_funct(image_in, model_p):\n",
    "    image_expanded = np.expand_dims(image_in, axis = 0)\n",
    "    return model_p.predict(image_expanded)\n",
    "\n",
    "out_img = pred_funct(img_data, model).squeeze()\n",
    "fig, (input_figure, output_figure) = plt.subplots(1, 2, figsize = (20, 15))\n",
    "input_figure.imshow(selected_image)\n",
    "out_mask = 128.0 * (1.0 - out_img[:,:,0] + out_img[:,:,1])\n",
    "output_figure.imshow(out_mask.astype(np.uint8), cmap = 'gray')\n",
    "\n",
    "numpy_in = os.path.join(sample_test_dir, \"in_vgg.npy\")\n",
    "np.save(numpy_in, np.transpose(img_data,[1,0,2]))\n",
    "\n",
    "input_figure.set_title('Image')\n",
    "input_figure.set_axis_off()\n",
    "\n",
    "output_figure.set_title('Mask Image')\n",
    "output_figure.set_axis_off()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model, check_index):\n",
    "    input_layer_data = model.layers[0]\n",
    "    intermediate_layer_data = model.layers[check_index-1]\n",
    "    output_layer_data = model.layers[check_index+1]\n",
    "\n",
    "    model1 = Model(input_layer_data.input, intermediate_layer_data.output)\n",
    "    model2 = Model(intermediate_layer_data.output, output_layer_data.output)\n",
    "\n",
    "    print(input_layer_data.name)\n",
    "    print(intermediate_layer_data.name)\n",
    "    print(output_layer_data.name)\n",
    "\n",
    "    print(input_layer_data.input.shape[1:])\n",
    "    print(intermediate_layer_data.output.shape[1:])\n",
    "    print(output_layer_data.output.shape[1:])\n",
    "    return model1, model2\n",
    "def write_csv3d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for l in range(output_data.shape[3]):\n",
    "            for k in range(output_data.shape[2]):\n",
    "                for j in range(output_data.shape[1]):\n",
    "                    for i in range(output_data.shape[0]):\n",
    "                        # Join the text data in each row with commas\n",
    "                        value = output_data[i, j, k, l]\n",
    "                        # Write the row data to the output file\n",
    "                        if i == output_data.shape[1] - 1:\n",
    "                            file.write(\"{:.4f}\".format(value))\n",
    "                        else:\n",
    "                            file.write(\"{:.4f},\".format(value))\n",
    "                    file.write(\"\\n\")\n",
    "            \n",
    "def write_csv(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for k in range(output_data.shape[2]):\n",
    "            for j in range(output_data.shape[0]):\n",
    "                for i in range(output_data.shape[1]):\n",
    "                    # Join the text data in each row with commas\n",
    "                    value = output_data[j, i, k]\n",
    "                    # Write the row data to the output file\n",
    "                    if i == output_data.shape[1] - 1:\n",
    "                        file.write(\"{:.4f}\".format(value))\n",
    "                    else:\n",
    "                        file.write(\"{:.4f},\".format(value))\n",
    "                file.write(\"\\n\")\n",
    "def runthis(mode1, model2, img_data):\n",
    "    intermediate_output = pred_funct(img_data, model1).squeeze()\n",
    "    output = pred_funct(intermediate_output, model2).squeeze()\n",
    "    print(\"-----------------\")\n",
    "    for k in range(3):\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(img_data[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(img_data.shape)\n",
    "    print(\"-----------------\")\n",
    "    for k in range(min(3, intermediate_output.shape[2])): \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(intermediate_output[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(intermediate_output.shape)\n",
    "    write_csv(intermediate_output, \"in\")     \n",
    "    print(\"-----------------\")\n",
    "    for k in range(min(3, output.shape[2])): \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                print(output[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "    print(output.shape)  \n",
    "    write_csv(output, \"out\")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(model.layers))\n",
    "check_index = 1\n",
    "for index, layer in enumerate(model.layers):\n",
    "    if layer.name in [\"block1_conv1\"]:\n",
    "        print(index, layer.name)\n",
    "        model1, model2 = generate_model(model, index)\n",
    "        runthis(model1, model2, img_data)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.random.uniform(low=0.0, high=10.0, size = input_layer_data.input.shape[1:]).astype(np.int32).astype(np.float32)\n",
    "#img_data = np.ones(input_layer_data.input.shape[1:], dtype=np.float32)\n",
    "#img_data = np.zeros(input_layer_data.input.shape[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"batch_normalization\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_mean.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_variance.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "else:\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_bias.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    \n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_weights.npy')\n",
    "data = np.load(path_to_saved)\n",
    "if 'depth' in filer_name:\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 0, 0], end=\", \")\n",
    "        print()\n",
    "    print()\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            print(data[i, j, 1, 0], end=\", \")\n",
    "        print()\n",
    "else:\n",
    "    for l in range(2):\n",
    "        for k in range(4):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    print(data[j, i, k ,l], end=\", \")\n",
    "                print()\n",
    "            print(\"\")\n",
    "        print(\"+++++++++\")\n",
    "    print(\"\\n--------------\")\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filer_name = \"expanded_conv_project\"\n",
    "filer_name = \"Conv1\"\n",
    "numpy_in = os.path.join(sample_test_dir, filer_name + \"_in.npy\")\n",
    "np.save(numpy_in, np.transpose(img_data,[1,0,2]))\n",
    "print(numpy_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"conv2d\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{file_name}.npy')\n",
    "data = np.load(path_to_saved)\n",
    "\n",
    "for l in range(2):\n",
    "    for k in range(2):\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                print(data[j, i, k ,l], end=\", \")\n",
    "            print()\n",
    "        print(\"\")\n",
    "    print(\"+++++++++\")\n",
    "print(data.shape)\n",
    "print(\"\\n--------------\")\n",
    "\n",
    "write_csv3d(data, \"filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "VggUnet.ipynb",
   "provenance": [
    {
     "file_id": "1U-lQR00_5mk5jPArZSNTgqP0a8Vmlodz",
     "timestamp": 1606041309723
    },
    {
     "file_id": "https://github.com/ashishpatel26/Semantic-Segmentation-Keras-Tensorflow-Example/blob/main/Satellight_Image_Semantic_Segmentation_From_Scratch.ipynb",
     "timestamp": 1606031989735
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07ca0455627c47a78b5a99c64c285abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73d95f2e29c448bda8505e09581e1c0e",
      "placeholder": "​",
      "style": "IPY_MODEL_c5c70e40ef4e4006b90c03d85f04ca97",
      "value": " 61/? [03:01&lt;00:00,  2.97s/it]"
     }
    },
    "19bd521b63ea4fbaaccb116fcd021a70": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "480ceb01acd74b91b57950f80b557e1e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dd4aa10b57e46ac9cb98bb92100c728": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f922376e800e4683b455f45b2eb9d9c8",
       "IPY_MODEL_07ca0455627c47a78b5a99c64c285abd"
      ],
      "layout": "IPY_MODEL_480ceb01acd74b91b57950f80b557e1e"
     }
    },
    "73d95f2e29c448bda8505e09581e1c0e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "951347637fa544d790335e75bd622fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c5c70e40ef4e4006b90c03d85f04ca97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f922376e800e4683b455f45b2eb9d9c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19bd521b63ea4fbaaccb116fcd021a70",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_951347637fa544d790335e75bd622fac",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
